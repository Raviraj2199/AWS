# -*- coding: utf-8 -*-
"""tej-glue-notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JJcRYDLYj6kxrGjcUzj78JDMEECwu47Q

# AWS Glue Studio Notebook
##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.

#### Optional: Run this cell to see available notebook commands ("magics").
"""

# Commented out IPython magic to ensure Python compatibility.
# %help

"""####  Run this cell to set up and start your interactive session.

"""

# Commented out IPython magic to ensure Python compatibility.
# %idle_timeout 2880
# %glue_version 3.0
# %worker_type G.1X
# %number_of_workers 5

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
  
sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

"""#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema

"""

dyf = glue_Context.create_dynamic_frame.from_catalog(database='customerstandalone', table_name='2023')
dyf.printSchema()

"""#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data

"""

df = dyf.toDF()
df.show()

"""#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog

"""

s3output = glue_context.getSink(
  path="s3://bucket_name/folder_name",
  connection_type="s3",
  updateBehavior="UPDATE_IN_DATABASE",
  partitionKeys=[],
  compression="snappy",
  enableUpdateCatalog=True,
  transformation_ctx="s3output",
)
s3output.setCatalogInfo(
  catalogDatabase="demo", catalogTableName="populations"
)
s3output.setFormat("glueparquet")
s3output.writeFrame(DyF)





import sys
from awsglue.transforms import *

from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

glueContext = GlueContext(SparkContext.getOrCreate())

customerDDf=glueContext.create_dynamic_frame.from_catalog(database='customerstandalone',table_name='2023')

customerDDf.printSchema()

customerDDf.toDF().show()

customerDf=customerDDf.toDF()

customerDf.head(5)

customerDf.show(5)

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,DoubleType

schema = StructType([ 
    StructField("cust_id",IntegerType(),True), 
    StructField("name",StringType(),True), 
    StructField("gender",StringType(),True), 
    StructField("date", StringType(), True), 
    StructField("address", StringType(), True), 
    StructField("distance", IntegerType(), True),
    StructField("amount", DoubleType(), True),
    StructField("p0", IntegerType(), True),
    StructField("p1", IntegerType(), True),
    StructField("p2", IntegerType(), True)
    
  ])

customerDf_proper= spark.createDataFrame(data=customerDDf,schema=schema)

customerDf_proper

customerDDf.toDF().show(5)

customerDDf=customerDDf.withColumnRenamed("col0","cust_id")

customerDDf=customerDDf.withColumnRenamed("col1","name")

customerDDf=customerDDf.withColumnRenamed("col2","gender").withColumnRenamed("col3","date").withColumnRenamed("col4","address")

customerDDf=customerDDf.withColumnRenamed("col5","time").withColumnRenamed("col6","distance").withColumnRenamed("col7","amount")

customerDDf.toDF().show(5)

